{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# DocSAF Demo Notebook\n",
        "\n",
        "This notebook demonstrates the DocSAF Phase 0 vertical slice:\n",
        "- Load a document image\n",
        "- Extract text via OCR  \n",
        "- Compute cross-modal saliency\n",
        "- Apply attenuation field with two knobs: `alpha` and `radius`\n",
        "- Generate adversarial image\n",
        "\n",
        "**Two tunables only:** `alpha` (field strength) and `radius` (blur kernel size)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from pathlib import Path\n",
        "\n",
        "# DocSAF imports\n",
        "from src.docsaf.utils import load_config, pil_to_tensor, tensor_to_pil\n",
        "from src.docsaf.surrogates import load_embedder\n",
        "from src.docsaf.ocr import ocr_read\n",
        "from src.docsaf.saliency import compute_gradient_saliency\n",
        "from src.docsaf.field import apply_field_safe\n",
        "from src.docsaf.pdf_io import pdf_to_pil, is_pdf_file\n",
        "\n",
        "print(\"DocSAF imports successful!\")\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Load Configuration and Image\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load config\n",
        "config = load_config(\"configs/default.yaml\")\n",
        "print(f\"Config alpha: {config['alpha']}, radius: {config['radius']}\")\n",
        "\n",
        "# Load demo image (replace with your image path)\n",
        "image_path = \"demo/sample_doc.png\"  # Change this to your demo image\n",
        "\n",
        "if is_pdf_file(image_path):\n",
        "    print(\"Loading PDF page...\")\n",
        "    pil_image = pdf_to_pil(image_path, page=0, zoom=2.0)\n",
        "else:\n",
        "    print(\"Loading image...\")\n",
        "    pil_image = Image.open(image_path).convert(\"RGB\")\n",
        "\n",
        "# Convert to tensor\n",
        "x_orig = pil_to_tensor(pil_image, device)\n",
        "\n",
        "# Display original image\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.imshow(pil_image)\n",
        "plt.title(\"Original Document Image\")\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "print(f\"Image shape: {x_orig.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Extract Text via OCR\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract text using OCR\n",
        "img_array = np.array(pil_image)\n",
        "ocr_backend = config.get(\"ocr\", \"easyocr\")\n",
        "extracted_text = ocr_read(img_array, backend=ocr_backend)\n",
        "\n",
        "print(f\"OCR Backend: {ocr_backend}\")\n",
        "print(f\"Extracted Text ({len(extracted_text)} chars):\")\n",
        "print(f\"'{extracted_text[:200]}{'...' if len(extracted_text) > 200 else ''}'\")\n",
        "\n",
        "# Use fallback text if OCR fails\n",
        "if not extracted_text.strip():\n",
        "    extracted_text = \"document text content\"\n",
        "    print(\"Using fallback text for demo\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Compute Cross-Modal Saliency → Apply Attenuation Field → Compare Results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load embedder model\n",
        "surrogate_specs = config.get(\"surrogates\", [\"openclip:ViT-L-14@336\"])\n",
        "embedder_spec = surrogate_specs[0] if isinstance(surrogate_specs, list) else surrogate_specs\n",
        "embedder = load_embedder(embedder_spec, device)\n",
        "print(f\"Loaded embedder: {embedder_spec}\")\n",
        "\n",
        "# Compute gradient saliency\n",
        "x_input = x_orig.clone().requires_grad_(True)\n",
        "original_alignment, saliency_map = compute_gradient_saliency(\n",
        "    embedder, x_input, extracted_text, normalize=True\n",
        ")\n",
        "\n",
        "print(f\"Original CLIP alignment score: {original_alignment:.4f}\")\n",
        "\n",
        "# Apply attenuation field with the two knobs\n",
        "alpha = config['alpha']  # Field strength\n",
        "radius = config['radius']  # Blur radius\n",
        "print(f\"Applying attenuation field with alpha={alpha}, radius={radius}\")\n",
        "\n",
        "x_adv = apply_field_safe(x_orig, saliency_map, alpha, radius)\n",
        "\n",
        "# Compute adversarial alignment score\n",
        "with torch.no_grad():\n",
        "    adv_alignment, _ = compute_gradient_saliency(embedder, x_adv, extracted_text)\n",
        "\n",
        "alignment_drop = original_alignment - adv_alignment\n",
        "print(f\"Adversarial CLIP alignment score: {adv_alignment:.4f}\")\n",
        "print(f\"Alignment drop: {alignment_drop:.4f}\")\n",
        "\n",
        "# Convert to PIL for visualization\n",
        "pil_adv = tensor_to_pil(x_adv)\n",
        "\n",
        "# Side-by-side comparison\n",
        "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
        "\n",
        "# Original image\n",
        "axes[0,0].imshow(pil_image)\n",
        "axes[0,0].set_title(f\"Original\\\\nCLIP Score: {original_alignment:.4f}\")\n",
        "axes[0,0].axis('off')\n",
        "\n",
        "# Saliency heatmap  \n",
        "sal_cpu = saliency_map.squeeze().cpu().numpy()\n",
        "im1 = axes[0,1].imshow(sal_cpu, cmap='hot', interpolation='bilinear')\n",
        "axes[0,1].set_title(f\"Saliency Map\")\n",
        "axes[0,1].axis('off')\n",
        "plt.colorbar(im1, ax=axes[0,1], fraction=0.046)\n",
        "\n",
        "# Saliency overlay\n",
        "axes[1,0].imshow(pil_image)\n",
        "axes[1,0].imshow(sal_cpu, cmap='hot', alpha=0.4, interpolation='bilinear')\n",
        "axes[1,0].set_title(\"Saliency Overlay\")\n",
        "axes[1,0].axis('off')\n",
        "\n",
        "# Adversarial result\n",
        "axes[1,1].imshow(pil_adv)\n",
        "axes[1,1].set_title(f\"Adversarial (α={alpha}, r={radius})\\\\nCLIP Score: {adv_alignment:.4f}\\\\nDrop: {alignment_drop:.4f}\")\n",
        "axes[1,1].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Save adversarial image\n",
        "output_path = Path(image_path).parent / f\"{Path(image_path).stem}_adv.png\"\n",
        "pil_adv.save(output_path)\n",
        "print(f\"Saved adversarial image to: {output_path}\")\n",
        "\n",
        "print(\"\\\\n=== DocSAF Phase 0 Demo Complete ===\")\n",
        "print(f\"Two tunables used: alpha={alpha}, radius={radius}\")\n",
        "print(f\"Success: {alignment_drop > 0.01}\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
